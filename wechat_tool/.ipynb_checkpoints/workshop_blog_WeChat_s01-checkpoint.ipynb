{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何使用微信聊天机器 / Kudos Data blog workshop of WeChat chat-bot\n",
    "\n",
    "### Part I 问答机制示范\n",
    "自动接收、回复消息。支持私信，群组，公众号等\n",
    "\n",
    "文字，图片，语音的接收和回复。\n",
    "\n",
    "\n",
    "### Part II 图像识别\n",
    "识别图片消息中的物体名字\n",
    "\n",
    "识别人脸\n",
    "\n",
    "\n",
    "### Part III 自然语言处理\n",
    "语音消息的识别，转换成文字\n",
    "\n",
    "消息文字转成语音\n",
    "\n",
    "消息的多语言互译\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 酷豆数据科学 www.KudosData.com\n",
    "By: Sam GU Zhan\n",
    "\n",
    "April, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install WeChat API library if not installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wxpy\n",
      "  Downloading wxpy-0.3.5.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-build-fsPUfM/wxpy/setup.py\", line 8, in <module>\n",
      "        with open('wxpy/__init__.py', encoding='utf-8') as fp:\n",
      "    TypeError: 'encoding' is an invalid keyword argument for this function\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-fsPUfM/wxpy/\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U wxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting itchat\n",
      "  Downloading itchat-1.3.5-py2.py3-none-any.whl\n",
      "Collecting pyqrcode (from itchat)\n",
      "  Downloading PyQRCode-1.2.1.zip (41kB)\n",
      "\u001b[K    100% |################################| 51kB 2.1MB/s \n",
      "\u001b[?25hCollecting requests (from itchat)\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% |################################| 593kB 1.4MB/s \n",
      "\u001b[?25hCollecting pypng (from itchat)\n",
      "  Downloading pypng-0.0.18.tar.gz (377kB)\n",
      "\u001b[K    100% |################################| 378kB 2.1MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyqrcode, pypng\n",
      "  Running setup.py bdist_wheel for pyqrcode ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/79/84/80/5909ca01732597bfd7d04d368689dc1c84736f9314fed2003a\n",
      "  Running setup.py bdist_wheel for pypng ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6d/f0/c9/6e31bc9ec080ed16c9b84b305c55716ea227026904af2d52f1\n",
      "Successfully built pyqrcode pypng\n",
      "Installing collected packages: pyqrcode, requests, pypng, itchat\n",
      "  Found existing installation: requests 2.9.1\n",
      "    Uninstalling requests-2.9.1:\n",
      "      Successfully uninstalled requests-2.9.1\n",
      "Successfully installed itchat-1.3.5 pypng-0.0.18 pyqrcode-1.2.1 requests-2.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U itchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: requests in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: pyqrcode in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: pypng in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: Pillow in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already up-to-date: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow)\n",
      "Collecting wxbot\n",
      "\u001b[31m  Could not find a version that satisfies the requirement wxbot (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for wxbot\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U requests\n",
    "!pip install -U pyqrcode\n",
    "!pip install -U pypng\n",
    "!pip install -U Pillow\n",
    "!pip install -U wxbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "█\r"
     ]
    }
   ],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import time\n",
    "from wxpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "from __future__ import division\n",
    "import re\n",
    "\n",
    "# Python2 unicode & float-division support:\n",
    "# from __future__ import unicode_literals, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cell magic `%%matplotlib` not found (But line magic `%matplotlib` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import io\n",
    "\n",
    "# 中文字符和语言处理库\n",
    "import jieba\n",
    "\n",
    "# 机器学习库 sklearn 分类学习模型库\n",
    "#from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer # 数据结构变换：把 Dict 转换为 稀疏矩阵\n",
    "# from sklearn.linear_model import LogisticRegression  # 逻辑回归分类模型\n",
    "# from sklearn.pipeline import make_pipeline # 封装机器学习模型流程\n",
    "# from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 中文显示设置\n",
    "from pylab import *  \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei'] # 指定默认字体  \n",
    "mpl.rcParams['axes.unicode_minus'] = False # 解决保存图像是负号'-'显示为方块的问题 \n",
    "mpl.rcParams['font.size'] = 14 # 设置字体大小\n",
    "\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_tokenizer(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "\n",
    "    return seg_str\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "# def KudosData_word_tokenizer(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = ' '.join(seg_token)\n",
    "#     return seg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# 中文分词功能小函数， 输出 字符串， 各词组由空格分隔\n",
    "def KudosData_word_count(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "    seg_count = pd.value_counts(str(seg_str).lower().split(' '))\n",
    "    seg_count = seg_count.to_dict() \n",
    "    seg_count.pop('', None) # remove EMPTY dict key: ''\n",
    "#     输出 dictionary： { key 词组， value 计数 }\n",
    "    #     return seg_count.to_dict()\n",
    "    return seg_count\n",
    "\n",
    "# Python2\n",
    "# 中文分词功能小函数， 输出 dictionary： { key 词组， value 计数 }\n",
    "# def KudosData_word_count(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = '^'.join(seg_token)\n",
    "#     seg_count = pd.value_counts(seg_str.lower().split('^'))\n",
    "#     return seg_count.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process Unicode text input\n",
    "with io.open('input_text.txt','r',encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "title = '''\n",
    "<Dummy Title>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sentence(text):\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linebreak_conversion_win_linux(text):\n",
    "    text = re.sub(r'\\r', '', text) # remove one or more Windows-line-break\n",
    "    text = re.sub(r'\\u3000', ' ', text) # convert white space: \\u3000    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_1(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    text = re.sub(r'\\f+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r'\\v+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    \n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_2(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\n+', ' ', text) # Change one or more \\n to a Space, this is to merge sentences within paragraph\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    text = re.sub(r'(\\^\\*\\#)( +)(\\#\\*\\^)', '^*##*^', text) # remove one or more Spaces between Paragraph-Tags or Sentence-Tags\n",
    "    \n",
    "    text = re.sub(r'(\\#\\*\\^S\\^\\*\\#)+', '#*^S^*#', text) # merge two or more sentence-Tags -> 1 Sentence-Tag\n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "    \n",
    "    # remove a Sentence-Tag immediately before a Paragraph-Tag\n",
    "    text = re.sub(r'(\\#\\*\\^S\\^\\*\\#)( *)(\\#\\*\\^P\\^\\*\\#)', '#*^P^*#', text) \n",
    "\n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define Paragraph-Tag =  \n",
    "#   #*^P^*#\n",
    "\n",
    "### Define Sentence-Tag =  \n",
    "#   #*^S^*#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each paragraph\n",
    "def tag_paragraph(text):\n",
    "    text = re.sub(r'((\\n ) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Spaces\n",
    "    text = re.sub(r'((\\n\\t) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Tabs\n",
    "    text = re.sub(r'(\\n( *)\\n)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + zero or more Spaces + \\n\n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each sentence\n",
    "def tag_sentence(text):\n",
    "    text = re.sub(r'。+', '。#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'！+', '！#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'\\？+', '？#*^S^*#', text) # Tag sentence - Chinese\n",
    "#     text = re.sub(r'；+', '；#*^S^*#', text) # Tag sentence - Chinese\n",
    "\n",
    "    # 2017 MAR 24\n",
    "    text = re.sub(r'(\\.)( +)', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'(!)( +)', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?( +)', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r'(;)( +)', ';#*^S^*#', text) # Tag sentence - English\n",
    "\n",
    "    text = re.sub(r'\\.\\n', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'!\\n', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?\\n', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r';\\n', ';#*^S^*#', text) # Tag sentence - English\n",
    "    \n",
    "    # remove a Sentence-Tag immediately before an ending quotation\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#’', '’#*^S^*#', text) # Chinese ’\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#”', '”#*^S^*#', text) # Chinese ”\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\\'', '\\'#*^S^*#', text) # English '\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\"', '\"#*^S^*#', text) # English \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = linebreak_conversion_win_linux(content)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = tag_paragraph(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34086\n"
     ]
    }
   ],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_1(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34083\n"
     ]
    }
   ],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = tag_sentence(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44019\n"
     ]
    }
   ],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_2(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42495\n"
     ]
    }
   ],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transfer tagged text to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a text into paragraphs\n",
    "def split_article_to_paragraphs(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^P^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a paragraph into sentences\n",
    "def split_paragraph_to_sentences(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^S^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st loop Paragraphs list, 2nd loop Sentences list\n",
    "# create a few new columns, then write into dataframe, together with original Sentence string\n",
    "\n",
    "# define empty dataframe:\n",
    "df_article = pd.DataFrame(columns=('sentence', \n",
    "                                   'word_count', # sentence word count, including punctuations \n",
    "                                   'sentence_id', # unique sentence s/n within an article\n",
    "                                   'sentence_id_paragraph',  # sentence s/n within a paragraph \n",
    "                                   'paragraph_id', \n",
    "                                   'class_rank', \n",
    "                                   'score_word', # score based on word tf-idf\n",
    "                                   'score_sentence', # score based on intersection of sentence pairs\n",
    "                                   'score_word_norm', # Normalized score\n",
    "                                   'score_sentence_norm', # Normalized score\n",
    "                                   'score',\n",
    "                                  ))\n",
    "df_sentence_id = 0\n",
    "\n",
    "# split_article_to_paragraphs:\n",
    "article_paragraphs = split_article_to_paragraphs(content_format)\n",
    "\n",
    "for i in range(0, len(article_paragraphs)):\n",
    "    # split_paragraph_to_sentences:\n",
    "    article_paragraphs_sentences = split_paragraph_to_sentences(article_paragraphs[i].strip())\n",
    "\n",
    "    for j in range(0, len(article_paragraphs_sentences)):\n",
    "        if article_paragraphs_sentences[j].strip() != '':\n",
    "            df_sentence_id = df_sentence_id + 1\n",
    "            # write to dataframe:\n",
    "            df_article.loc[len(df_article)] = [article_paragraphs_sentences[j].strip(), \n",
    "                                               len(article_paragraphs_sentences[j].strip()), \n",
    "                                               df_sentence_id, \n",
    "                                               j+1, \n",
    "                                               i+1, \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of Article :  《黄金时代》 王小波\n"
     ]
    }
   ],
   "source": [
    "# assume the 1st sentence as Title of Article\n",
    "\n",
    "title = df_article['sentence'][0]\n",
    "print('Title of Article : ', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                《黄金时代》 王小波\n",
       "1                         一\n",
       "2            我二十一岁时，正在云南插队。\n",
       "3    陈清扬当时二十六岁，就在我插队的地方当医生。\n",
       "4          我在山下十四队，她在山上十五队。\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_article['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting uuid of QR code.\n",
      "INFO:itchat:Getting uuid of QR code.\n",
      "Downloading QR code.\n",
      "INFO:itchat:Downloading QR code.\n",
      "Please scan the QR code to log in.\n",
      "INFO:itchat:Please scan the QR code to log in.\n",
      "Please press confirm on your phone.\n",
      "INFO:itchat:Please press confirm on your phone.\n",
      "Loading the contact, this may take a little while.\n",
      "INFO:itchat:Loading the contact, this may take a little while.\n",
      "Login successfully as 白黑\n",
      "INFO:itchat:Login successfully as 白黑\n"
     ]
    }
   ],
   "source": [
    "# 初始化机器人，扫码登陆\n",
    "bot = Bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Friend: 郝素素>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 搜索好友名称\n",
    "# my_friend = bot.friends().search('hss7777777')[0]\n",
    "my_friend = bot.friends().search('郝素素')[0]\n",
    "# my_friend = bot.friends().search('白黑')[0]\n",
    "my_friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_friend.send('<程序自动发送>\\n<第%d条，共%d条>\\n%s' % (1, len(df_article['sentence']), df_article['sentence'][100]))\n",
    "# my_friend.send('<程序自动发送>\\n恭喜恭喜！\\n红包拿来~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rec_id = 730\n",
      "success rec_id = 731\n",
      "success rec_id = 732\n",
      "success rec_id = 733"
     ]
    }
   ],
   "source": [
    "# 发送文本给好友\n",
    "count = len(df_article['sentence'])\n",
    "# count = 5\n",
    "for i in range(729, count):\n",
    "    try:\n",
    "        my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "        print ('success rec_id = %d' % (i+1))\n",
    "        time.sleep(np.random.randint(low = 5, high = 20))\n",
    "    except:\n",
    "        tmp_rand_int = np.random.randint(low = 70, high = 130)\n",
    "        print ('retry   rec_id = %d, after %d seconds... ' % (i+1, tmp_rand_int))\n",
    "        time.sleep(tmp_rand_int)\n",
    "        try:\n",
    "            my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "            print ('success rec_id = %d' % (i+1))\n",
    "            time.sleep(np.random.randint(low = 5, high = 20))\n",
    "        except:\n",
    "            tmp_rand_int = np.random.randint(low = 70, high = 130)\n",
    "            print ('retry   rec_id = %d, after %d seconds... ' % (i+1, tmp_rand_int))\n",
    "            time.sleep(tmp_rand_int)\n",
    "            my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "            print ('success rec_id = %d' % (i+1))\n",
    "            time.sleep(np.random.randint(low = 5, high = 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    try:\n",
    "        my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "        print ('success rec_id = %d' % (i+1))\n",
    "        time.sleep(np.random.randint(low = 5, high = 20))\n",
    "    except:\n",
    "        tmp_rand_int = np.random.randint(low = 70, high = 130)\n",
    "        print ('retry   rec_id = %d, after %d seconds... ' % (i+1, tmp_rand_int))\n",
    "        time.sleep(tmp_rand_int)\n",
    "        my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "        print ('success rec_id = %d' % (i+1))\n",
    "        time.sleep(np.random.randint(low = 5, high = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 发送文本给好友\n",
    "count = len(df_article['sentence'])\n",
    "# count = 10\n",
    "for i in range(0, count):\n",
    "    my_friend.send('<第%d条，共%d条>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "    print (i+1)\n",
    "#     my_friend.send('<程序自动发送> \\n恭喜恭喜！\\n红包拿来~')\n",
    "    time.sleep(np.random.randint(low = 10, high = 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KudosData_word_tokenizer\n",
    "df_article['sentence_tokenized'] = df_article['sentence'].apply(lambda x: KudosData_word_tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure no empty sentences:\n",
    "print('Number of empty sentences in dataframe: %d ' % len(df_article[df_article['sentence_tokenized'] == '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid empty sentences\n",
    "print(len(df_article))\n",
    "df_article=df_article[df_article['sentence_tokenized'] != '']\n",
    "df_article = df_article.sort_values(by=['sentence_id'],).reset_index(drop=True)\n",
    "print(len(df_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KudosData_word_count\n",
    "df_article['sentence_tf'] = df_article['sentence'].apply(lambda x: KudosData_word_count(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = df_article['sentence_tokenized']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# my_stopword_list = ['and','to','the','of', 'in']\n",
    "#vectorizer = TfidfVectorizer(stop_words=my_stopword_list)\n",
    "\n",
    "# choice of no nomalization of tfidf output (not recommended)\n",
    "#vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# TF-IDF score\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# IDF score\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# TF is in df_article[['sentence_tf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 把TF-iDF数值赋予相对应的词组\n",
    "tfidf = tfidf.tocsr()\n",
    "\n",
    "n_docs = tfidf.shape[0]\n",
    "tfidftables = [{} for _ in range(n_docs)]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, j in zip(*tfidf.nonzero()):\n",
    "    tfidftables[i][terms[j]] = tfidf[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document-Term-Matrix's TF-IDF matrix size:\n",
    "print ('This tfidf matrix is a very large table: [ %d rows/docs X %d columns/words ]' \n",
    "       % (tfidf.shape[0], tfidf.shape[1]))\n",
    "print ('It contains %d eliments: one score per word per document !'\n",
    "       % (tfidf.shape[0] * tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add tfidf score into dataframe \n",
    "df_article['tfidf'] = tfidftables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[['sentence', 'sentence_tokenized', 'sentence_tf', 'tfidf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate importance score for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (1)\n",
    "### Calculate score_word for each sentence, based on sentence word_count tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment: use tf-idf and len(sentence_tokenized) to calculate score\n",
    "# tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "\n",
    "for i in range(0,len(df_article)):\n",
    "    if len(df_article['tfidf'][i]) == 0:\n",
    "        df_article['score_word'][i] = 0\n",
    "    else:\n",
    "        tmp_sum = 0\n",
    "        for key, values in df_article['tfidf'][i].items():\n",
    "            tmp_sum += values\n",
    "        \n",
    "        tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "        df_article['score_word'][i] = tmp_mean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (2)\n",
    "### Calculate score_sentence for each sentence, based on pair-wise sentence comparison/intersection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Caculate raw intersection score between pair of two sentences, from df_article['sentence_tokenized']\n",
    "def sentences_intersection(sent1tokenized, sent2tokenized):\n",
    "    # www.KudosData.com - Chinese\n",
    "    # split the sentence into words/tokens\n",
    "    s1 = set(sent1tokenized.split(\" \"))\n",
    "    s2 = set(sent2tokenized.split(\" \"))\n",
    "\n",
    "    # If there is not intersection, just return 0\n",
    "    if (len(s1) + len(s2)) == 0:\n",
    "        print('# If there is not intersection, just return 0')\n",
    "        return 0\n",
    "\n",
    "    # Normalize the result by the average number of words\n",
    "    return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate important score of every pair of sentences\n",
    "\n",
    "n = len(df_article['sentence_tokenized'])\n",
    "        \n",
    "# [Sam python 2.7 -> 3.4] values = [[0 for x in xrange(n)] for x in xrange(n)]\n",
    "df_score_raw_values = [[0 for x in range(n)] for x in range(n)]\n",
    "for i in range(0, n):\n",
    "    for j in range(0, n):\n",
    "        df_score_raw_values[i][j] = sentences_intersection(df_article['sentence_tokenized'][i], \n",
    "                                                           df_article['sentence_tokenized'][j])\n",
    "\n",
    "# The score of a sentence is the sum of all its intersection\n",
    "sentences_dic = {}\n",
    "\n",
    "for i in range(0, n):\n",
    "    df_score = 0\n",
    "    for j in range(0, n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        df_score += df_score_raw_values[i][j]\n",
    "    df_article['score_sentence'][i] = df_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data (Internal use,  not for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：Sentence word_count')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "# df_article['word_count'].value_counts().sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "df_article['word_count'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：Paragraph_id')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "df_article['paragraph_id'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：sentence_id_paragraph')  \n",
    "plt.ylabel(u'Y坐标：Sentence frequency')  \n",
    "df_article['sentence_id_paragraph'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：score_word')  \n",
    "plt.ylabel(u'Y坐标：frequency')  \n",
    "df_article['score_word'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 图表显示：\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'图')  \n",
    "plt.xlabel(u'X坐标：score_sentence')  \n",
    "plt.ylabel(u'Y坐标：frequency')  \n",
    "df_article['score_sentence'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score_word'] > 0.15) & (df_article['score_word'] < 0.25)]\n",
    "# df_article[(df_article['score_word'] > 0.2)].sort_values(by=['score_sentence', 'score_word'], ascending=[False, False,])\n",
    "# df_article[(df_article['score_sentence'] > 250)].sort_values(by=['score_word', 'score_sentence'], ascending=[False, False,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log(score_word)\n",
    "df_article['score_word_log'] = np.log(df_article['score_word'].astype('float64') + \n",
    "                                      df_article[df_article['score_word'] >0 ]['score_word'].min()/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_word_log - Zero mean, unit variance\n",
    "\n",
    "# df_article['score_word_norm'] = (df_article['score_word'] - df_article['score_word'].mean()) / df_article['score_word'].std()\n",
    "df_article['score_word_norm'] = (df_article['score_word_log'] - df_article['score_word_log'].mean()) / df_article['score_word_log'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_word_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_sentence - Zero mean, unit variance\n",
    "\n",
    "df_article['score_sentence_norm'] = (df_article['score_sentence'] - df_article['score_sentence'].mean()) / df_article['score_sentence'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_sentence_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate class_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score integration\n",
    "# df_article['score'] = (df_article['score_sentence_norm'] + df_article['score_word_norm']) / 2\n",
    "\n",
    "# Sam Gu: 23 Mar 2017 - Experiment found that the score_word, which is based on tf-idf, doesn't seem to work well.\n",
    "#                       score_word     tends to favor short sentences\n",
    "#                       score_sentence tends to favor long  sentences\n",
    "#                       Hence, here we use score_sentence only for final scoring.\n",
    "\n",
    "# df_article['score'] = (df_article['score_word'] + df_article['score_sentence'] ) / 2\n",
    "df_article['score'] = df_article['score_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Min-Max normalization:\n",
    "df_article['score'] = (df_article['score'] - df_article['score'].min()) / (df_article['score'].max() -df_article['score'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort firstly\n",
    "df_article = df_article.sort_values(by=['paragraph_id', 'score'], ascending=[True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Class_Rank\n",
    "\n",
    "current_class_rank = 0\n",
    "current_paragraph_id = 0\n",
    "\n",
    "for i in range(0, len(df_article)):\n",
    "    if df_article['paragraph_id'][i] != current_paragraph_id: # change of Paragraph, thus reset class_rank\n",
    "        current_class_rank = 1\n",
    "        current_paragraph_id = df_article['paragraph_id'][i]\n",
    "    else:\n",
    "        current_class_rank = current_class_rank + 1\n",
    "        \n",
    "    df_article['class_rank'][i] = current_class_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article = df_article.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_article[['sentence',\n",
    "           'paragraph_id',\n",
    "           'sentence_id_paragraph',\n",
    "           'class_rank',\n",
    "           'score',\n",
    "           'sentence_tokenized'\n",
    "          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score'] == 0) | (df_article['score'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract results based on user parameters:\n",
    "* Max number of words\n",
    "* % of original number of words\n",
    "* Max lines of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataframe copy\n",
    "# Currently, the two dataframes are exactly the same.\n",
    "df_article_internal = pd.DataFrame.copy(df_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "# total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "# print('total_words_original_article : ', total_words_original_article)\n",
    "# print('total_words_internal_article : ', total_words_internal_article)\n",
    "# print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sam Gu: experiment shows no major improvement to use code in this block:\n",
    "\n",
    "'''\n",
    "\n",
    "# Heuristic cleaning:\n",
    "# 1.Remove sentences, which has only one valid word. \n",
    "# 2.Remove paragraph, which has only single sentence.\n",
    "\n",
    "# 1.\n",
    "df_article_internal = df_article_internal[df_article_internal['sentence_tokenized'].map(len) > 1]\n",
    "print('*** www.KudosData.com *** Removed number of sentences, which has only one valid word : %d'\n",
    "      % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# 2.\n",
    "df_article_internal_paragraph = df_article_internal['paragraph_id'].value_counts().to_frame(name = 'sentence_count')\n",
    "df_article_internal_paragraph = df_article_internal_paragraph[df_article_internal_paragraph['sentence_count'] > 1]\n",
    "valid_paragraph_id = df_article_internal_paragraph.index.tolist()\n",
    "df_article_internal = df_article_internal[df_article_internal['paragraph_id'].isin(valid_paragraph_id)] \n",
    "print('*** www.KudosData.com *** Removed number of sentences in total : %d' % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article_internal = df_article_internal.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)\n",
    "# Above sort a must sort !!! for below processing:\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Accept user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# valid range: >= 0\n",
    "parm_max_word = 200\n",
    "\n",
    "# valid range: >= 0\n",
    "parm_max_sentence = 10\n",
    "\n",
    "# valid range: [0, 100%]\n",
    "parm_max_percent = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Validation of user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (isinstance(parm_max_word, int) | isinstance(parm_max_word, float)):\n",
    "    if parm_max_word >= 0:\n",
    "        print('!1! valid input parm_max_word : ', parm_max_word)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_word : ', parm_max_word)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_word : ', parm_max_word)\n",
    "\n",
    "if (isinstance(parm_max_sentence, int) | isinstance(parm_max_sentence, float)):\n",
    "    if parm_max_sentence >= 0:\n",
    "        print('!1! valid input parm_max_sentence : ', parm_max_sentence)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_sentence : ', parm_max_sentence)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_sentence : ', parm_max_sentence)\n",
    "\n",
    "if (isinstance(parm_max_percent, int) | isinstance(parm_max_percent, float)):\n",
    "    if parm_max_percent >= 0:\n",
    "        print('!1! valid input parm_max_percent : ', parm_max_percent)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_percent : ', parm_max_percent)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_percent : ', parm_max_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_percent\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word / total_words_original_article <= parm_max_percent:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "sum_current_percent = sum_current_word / total_words_original_article\n",
    "print('---------- cut by parm_max_percent :')\n",
    "print('sum_current_word  / total_words_original_article:', sum_current_percent)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_word\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word <= parm_max_word:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "print('---------- cut by parm_max_word :')\n",
    "print('sum_current_word :', sum_current_word)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_sentence\n",
    "\n",
    "cut_index = parm_max_sentence\n",
    "\n",
    "print('---------- cut by parm_max_sentence :')\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract top number of sentences as summary, based on: cut_index\n",
    "df_article_final = df_article_internal[0:cut_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort by original sentence order \n",
    "df_article_final = df_article_final.sort_values(by=['sentence_id'], ascending=[True])\n",
    "df_article_final[['sentence_id', 'sentence', 'score', 'class_rank', 'paragraph_id', 'sentence_id_paragraph']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "# total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "print('total_words_original_article : ', total_words_original_article)\n",
    "print('total_words_internal_article : ', total_words_internal_article)\n",
    "print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('\\n'.join(list(df_article_final['sentence'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with io.open('output_topic_summary.txt','w',encoding='utf8') as f:\n",
    "#     f.write(\"Original Length : %s\" % total_words_original_article)\n",
    "    f.write(\"No. Paragraphs  : %d\" % df_article_internal['paragraph_id'].max())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Original Length : %s\" % total_words_internal_article)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Summary  Length : %s\" % total_words_article_summary)\n",
    "    f.write(\"\\n\")\n",
    "#     f.write(\"Summary  Ratio  : %s %%\" % (100 * (sum_current_word / total_words_original_article)))\n",
    "    f.write(\"Summary  Ratio  : %.2f %%\" % (100 * (total_words_article_summary / total_words_internal_article)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Title of Article: %s\" % title)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write('\\n'.join(list(df_article_final['sentence'])))\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is to check if there is sentence with very few valid/real word, should have very low score.\n",
    "df_article[['sentence', 'word_count', 'sentence_tokenized', 'tfidf', 'score']][df_article['sentence_tokenized'].map(len) <= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
